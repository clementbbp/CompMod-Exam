---
title: "Information Integration Project"
author: "Clement Peters"
date: "5/1/2021"
output: html_document
---

#Loading Packages
```{r}
##Packages
pacman::p_load(tidymodels, tidyverse, stringr, dplyr, brms, bayesplot,
               loo,
               lme4, tidymodels, factoextra)

```

#Defining functions:
```{r}

Integrate_main <- function(Agents, i, Info){
  
  # #If true, check if memory is full
         for(j in 10:ncol(Agents)){

        #Check if memory slot is open
         if(Agents[i,j] == 0){
           j <- as.numeric(j)

           #If true, save the information to the empty memory space
           Agents[i,j] <- Info$xi[1]
           Agents[i,(j+1)] <- Info$yi[1]

            #Break out of loop and continue to next agent
            break
         }
           
        }
  
        #If full, sample memory space, and replace with new value (integrate)
        
        #which place in memory should the new info go
        
        #Sample which memory should be replaced. There are nMemo possible values
        sample_num <- as.character(sample(1:((ncol(Agents)-9)/2), size=1)) 
        
        #Now i have the location (sample_num), I just need to plot in the new                   information
        colnames_df <- colnames(Agents[i,])
       
        memory_to_replace <- as.data.frame(str_detect(colnames_df, sample_num))
        
        #Now I know the columns in which the new info should go. So I need to put it there
        column <- numeric()
        for(k in 1:nrow(memory_to_replace)){
          if(memory_to_replace[k,] == TRUE){
            k <- as.numeric(k)
            column <- c(column, k) # Saving the index of the columns
            #column <- as.list(column)
            
          }
        }
        x_coord <- column[[1]] # memory x coordinate column number
        y_coord <- column[[2]] # memory y coordinate column number
        
        # And now we save the new memory in the right location
        Agents[i,][,x_coord] <- Info$xi[1]
        Agents[i,][,y_coord] <- Info$yi[1]
        
        #Now memory is updated. Time to update location on attitude space
        
        #First reset agent position to 0 before updating memory
        Agents$x[i] <- 0
        Agents$y[i] <- 0
        
        #Because the location of the agent is dependent on its memory, it will always be zero to begin with because almost all memories are "neutral". I need to average only over the non-zero memories.
        
        #Number of non-zero columns per agent a.k.a. how many memory spaces are filled + other variables
        
         nonZero <- sum(apply(Agents[i,], 2, function(c)sum(c!=0)))
         nonZero <- as.numeric(nonZero)
        
        # Averaging the x-values over the number of memories (of value x)
        Agents$x[i] <-  (sum(Agents[i,seq(10, ncol(Agents), 2)])/((nonZero-7)/2)) # -9 because there are always those 9 non memory columns which are nonZero, assuming that x and y are non zero.
        
        # Averaging the y-values over the number of memories (of value y)
        Agents$y[i] <- (sum(Agents[i,seq(11, ncol(Agents), 2)])/((nonZero-7)/2)) 
       return(Agents)
      }

Integrate_social <- function(Agents, Social_group, s, Info){
  
  # #If true, check if memory is full
         for(v in 10:ncol(Social_group)){
        
        #Check if memory slot is open
         if(Social_group[s,v] == 0){
        
           #If true, save the information to the empty memory space
           Social_group[s,v] <- Info$xi[2]
           Social_group[s,(v+1)] <- Info$yi[2]
           
           #Break out of loop and continue to next agent
           break
         }
        }
  
        #If full, sample memory space, and replace with new value (integrate)
        
        #which place in memory should the new info go
        
        #Sample which memory should be replaced. There are nMemo possible values
        sample_num <- as.character(sample(1:((ncol(Social_group)-9)/2), size=1))
        
        #Now i have the location (sample_num), I just need to plot in the new                   information
        colnames_df <- colnames(Social_group[s,])
       
        memory_to_replace <- as.data.frame(str_detect(colnames_df, sample_num))
        
        #Now I know the columns in which the new info should go. So I need to put it there
        column_social <- numeric()
        for(m in 1:nrow(memory_to_replace)){ #Two rows to loop through: memory x and y
          if(memory_to_replace[m,] == TRUE){
            
            column_social <- c(column_social, m) # Saving the index of the columns
            column_social <- as.list(column_social)
            
          }
        }
        x_coord <- column_social[[1]] # memory x coordinate column number
        y_coord <- column_social[[2]] # memory y coordinate column number
        
        #Reset agent position to 0 before updating memory
        Social_group$x[s] <- 0
        Social_group$y[s] <- 0
        
        # And now we save the new memory in the right location
        Social_group[s,][,x_coord] <- Info$xi[2]
        Social_group[s,][,y_coord] <- Info$yi[2]
        
        #Now memory is updated. Time to update location on attitude space
        
        #Because the location of the agent is dependent on its memory, it will always be zero to begin with because almost all memories are "neutral". I need to average only over the non-zero memories.
        
        #Number of non-zero columns per agent
        nonZero <- sum(apply(Social_group[s,], 2, function(c)sum(c!=0)))
        
        # Averaging the x-values over the number of memories (of value x)
        Social_group$x[s] <- sum(Social_group[s,seq(10, ncol(Social_group), 2)])/((nonZero-7)/2) # -7 because there are always those 7 non memory columns which are nonZero (at this place in the loop x and y are always zero)
        
        # Averaging the y-values over the number of memories (of value y)
        Social_group$y[s] <- sum(Social_group[s,seq(11, ncol(Social_group), 2)])/((nonZero-7)/2) 
        
        return(Social_group)
        } #Done integrating


#Define Agents
GenPop <- function(nPop, nMemo, Lat_accept, Sharpness, nGroups){
  

Agent <- data.frame(AgentNo = 1:nPop, # agent number
                    x = 0, # y coordinate
                    y = 0, # x coordinate
                    Lat = Lat_accept, #latitude of acceptance
                    Sharpness = Sharpness,
                    Valence = 1,
                    No_of_Shares = 1,
                    stringsAsFactors = FALSE)

#This column can't be zero because I use it for calculations later
Agent$No_of_Shares <- as.numeric(Agent$No_of_Shares)

#Assigning each agent to a group
groups <- as.data.frame(matrix(0, nrow = nrow(Agent), ncol = 1))
groups <- rename(groups, Group = V1)

for(a in 1:nrow(groups)){
  
  #It's debatable whether there should be an equal amount of members in each group, or if it should be random. I'm going to start with random amounts (but roughly equal), but later I might change in to be able to control group dynamics.
  groups[a,] <- sample(LETTERS[1:nGroups], 1, replace = TRUE)
}

#Deciding a "sharing threshold" for each agent
sharing <- as.data.frame(matrix(0, nrow = nrow(Agent), ncol = 1))
sharing <- rename(sharing, Share = V1)

for(a in 1:nrow(groups)){
  
  #Each agent will have a value indicating how likely they are to share their point of     view to other agents of their group.
  sharing[a,] <- runif(1,0,2)
}

#Building memory as dataframe with nMemo*2 column (x & y coordinates)
memory <- as.data.frame(matrix(0, nrow = nrow(Agent), ncol = nMemo*2))

prefixX <- "memory x"
prefixY <- "memory y"
suffix <- seq(1:nMemo)

namesx <- paste(prefixX, suffix, sep = "")
namesy <- paste(prefixY, suffix, sep = "")


colnames(memory)[seq(1,ncol(memory),2)] <- c(namesx)
colnames(memory)[seq(2,ncol(memory),2)] <- c(namesy)

Agent <- cbind(Agent, sharing, groups, memory)

return(Agent) 
  }

#Function for creating a penalty for rejected information
#range01 <- function(x){(x-min(x))/(max(x)-min(x))}
range01 <- function(x){(x-0.0001)/(2.828427-0.0001)}

#Creating info dataframe
Info <- data.frame(Info_num = 1:2, # Two pieces of into: main and social
                   xi = 0, # x coordinate for info
                   yi = 0) # y coordinate for info


#Defining ABM function
ABM1 <- function(Agents, Iterations, Plot_freq, Bias, Social_posting){

  #Loop through time
  for(t in 1:Iterations){
    
    #You could argue that the (mainstream) information should have a biased distribution      since the attitudinal space represents a certain topic, and mainstream media is         unlikely to represent it in a completely random manner.
    if(Bias == "None"){
    
    #Creating neutral information
    Info$xi[1] <- runif(1, -1, 1) #No bias
    Info$yi[1] <- runif(1, -1, 1) #No bias
    
    }
  
   #Conditional statement for Bias
    if(Bias == "Positive"){
    
    #Creating biased information
    Info$xi[1] <- rnorm(1, 0.5, 1) #positive bias
    Info$yi[1] <- rnorm(1, 0.5, 1) #Positive bias
    
    #keeping the values between -1 and 1
    while(Info$xi[1] < -1 | Info$xi[1] > 1 | Info$yi[1] < -1 | Info$yi[1] > 1){ 
    Info$xi[1] <- rnorm(1, 0.5, 1) #positive bias
    Info$yi[1] <- rnorm(1, 0.5, 1) #Positive bias
    
    }
  }
    
    if(Bias == "Negative"){
    
    #Creating biased information
    Info$xi[1] <- rnorm(1, -0.5, 1) #negative bias
    Info$yi[1] <- rnorm(1, -0.5, 1) #negative bias
    
    #keeping the values between -1 and 1
    while(Info$xi[1] < -1 | Info$xi[1] > 1 | Info$yi[1] < -1 | Info$yi[1] > 1){ 
    Info$xi[1] <- rnorm(1, -0.5, 1) #negative bias
    Info$yi[1] <- rnorm(1, -0.5, 1) #negative bias
    
    }
    }
    
    #Loop through population
    for(i in sample(c(1:nrow(Agents)))){ #loop in random order
      
      #Find the acceptance latitude of agent
      Latitude_m <- Agents$Lat[i]
      
      #Find attitude sharpness of agent
      Sharpness_m <- Agents$Sharpness[i]
      
      #What is the distance between agent and info
      Distance_main <- sqrt( ((Agents$x[i] - Info$xi[1])^2) + ((Agents$y[i] -
                                                                  Info$yi[1])^2) )
      
      #What is the chance they they will integrate
      RandNum <- runif(1,0,1)
      if (RandNum < ((Latitude_m^Sharpness_m)/
                     (Distance_main^Sharpness_m+Latitude_m^Sharpness_m))){
        
        #if true, integrate information
        Agents <- Integrate_main(Agents = Agents,
                                 i = i,
                                 Info = Info) 
        
      } else{ #Contrast effect:
        
        if(Distance_main > Lat_reject){
          
        #Check if information is positive or negative, then pull in opposite direction:
            if (Info$xi[1] >= 0){
            
            #pull valence in opposite direction
            Info$xi[1] <- (Info$xi[1]) - ((range01(Distance_main))*2) 
            
          } else{ #If Info$xi[1] < 0 (negative)
            
            #Pull valence in opposite direction
            Info$xi[1] <- (Info$xi[1]) - (-(range01(Distance_main))*2)
          }
          
          #Same thing for Y:
          if (Info$yi[1] >= 0){
            
            #pull valence in opposite direction
            
            Info$yi[1] <- (Info$yi[1]) - ((range01(Distance_main))*2) 
            
          } else{ #If Info$yi[1] < 0 (negative)
            
            #Pull valence in opposite direction
            Info$yi[1] <- (Info$yi[1]) - (-(range01(Distance_main))*2)
          } #Done implementing boomerang effect
        
          #Integrate new and penalized information:
          Agents <- Integrate_main(Agents = Agents, 
                                   i = i, 
                                   Info = Info)
        }
      } #Done integrating mainstream information 
      
     #Determine the absolute valency of agent "opinion" 
      Agents$Valence[i] <- sqrt((Agents$x[i])^2 + (Agents$y[i])^2)
      
      #Turn Social_posting on/off
      if(Social_posting == TRUE){
      
      #For every agent, see if they reach over the sharing threshold. Agents with high sharing value will have an easier time breaching the threshold  and will share more often.
        #Check if agent valency exceeds sharing threshold:
      if(Agents$Valence[i] > Agents$Share[i]){
        
        #Index forward No_of_Shares
        Agents$No_of_Shares[i] <-  Agents$No_of_Shares[i] + 1
        
        #If true, the agent will "post" it's location to members of it's group
        Info$xi[2] <- Agents$x[i]
        Info$yi[2] <- Agents$y[i]
        
        #Grab subset of agents who are in the same group as agent "i"
        Social_group <- subset(Agents, Agents$Group == Agents$Group[i])
        
        #Removing agent "i" from subset, so she doesn't integrate her own information
        Social_group <- Social_group %>% 
          filter(AgentNo != i)
        
        #Loop through each agent in social group as they try to integrate the information
        for(s in sample(c(1:nrow(Social_group)))){ #Loop in random order
          
          #Find the acceptance latitude of agent
          Latitude_social <- Social_group$Lat[s]
      
          #Find attitude sharpness of agent
          Sharpness_social <- Social_group$Sharpness[s]
          
          #distance between agent and social post
          Distance_social <- sqrt( ((Social_group$x[s] - Info$xi[2])^2) + 
                                     ((Social_group$y[s] -   Info$yi[2])^2) ) 
          #It would be easy to add a social bias here by making the distance shorter when in comes from an ingroup member. Maybe for a future study.
          
           #Each agent tries to integrate
          #What is the chance they they will integrate
        RandNum_social <- runif(1,0,1)
        if (RandNum_social <
            Latitude_social^Sharpness_social/(Distance_social^Sharpness_social+Latitude_social^Sharpness_social)){
        
        #if true, integrate
        Social_group <- Integrate_social(Agents = Agents, 
                                         Social_group = Social_group, 
                                         s = s,
                                         Info = Info)
       
      #Merge function doesn't work. But here I replace updated information (from Group) to original dataframe
      Agent_names <- Social_group$AgentNo
    for(x in Agent_names){
      
      #Check where the agent is in the original dataframe (AgentNo in Agents)
      if(Social_group$AgentNo[x] %in% Agents$AgentNo){
        
        #If true, replace old data new updated data
        Agents[Agents$AgentNo == x,] <- Social_group[Social_group$AgentNo == x,]
      
          }
        } #Done updating (merging) dataframe
      } else { #Adding boomerang effect
        
        if(Distance_social > Lat_reject){
          
          if (Info$xi[2] >= 0){
            Distance_social <- 0.5
            #pull valence in opposite direction
            Info$xi[2] <- (Info$xi[2]) - ((range01(Distance_social))*2)
            
          } else{ #If Info$xi[2] < 0 (negative)
            
            #Pull valence in opposite direction
            Info$xi[2] <- (Info$xi[2]) - (-(range01(Distance_social))*2)
          }
          
        # "Flip" y
          if (Info$yi[2] >= 0){
            
            #pull valence in opposite direction
            Info$yi[2] <- (Info$yi[2]) - ((range01(Distance_social))*2) 
            
          } else{ #If Info$xi[1] < 0 (negative)
            
            Info$yi[2] <- (Info$yi[2]) - (-(range01(Distance_social))*2)
          } #Done implementing boomerang effect
          
          #Integrate penalized information
          Social_group <- Integrate_social(Agents = Agents, 
                                           Social_group = Social_group, 
                                           s = s,
                                           Info = Info)
          
          #Replace updated information (from Group) to original dataframe
          Agent_names <- Social_group$AgentNo
          for(x in Agent_names){
            
            #Check where the agent is in the original dataframe (AgentNo in Agents)
            if(Social_group$AgentNo[x] %in% Agents$AgentNo){
              #If true, replace old data new updated data
              Agents[Agents$AgentNo == x,] <- Social_group[Social_group$AgentNo == x,]
              
          }
        } # Done merging
      
      } # Reject integrate social
    } # Done integrating rejection social
  } # Done looping through social group
  } #Agents valence > Agents Share
  
  
      } # Social posting == TRUE 
    } # Done looping through agents
    #Visualization:
    if(t %% Plot_freq == 0 | t == 1){

    plot <- Agents %>%
    ggplot(aes(x, y, color = Group)) + geom_point(aes(size = Share)) + xlim(-1, 1) + ylim(-1, 1) + theme(legend.position = "None") + ggtitle(paste("TickNo", t)) +
      scale_size(trans = 'reverse')
    
    print(plot)
    }
  } # Done looping through time
  Agents$No_of_Shares <- Agents$No_of_Shares - 1 # Subtracting one so it's the correct No_of_Shares
  
  return(Agents)
}
```

#Playing around with probability function
```{r}
D <- 0.5 #Latitude of acceptance: The distance at which probability of information is 50%

DR <- 1 #Latitude of rejection: The distance at which information exceeding this amount will be reversed and then integrated

d <- 0.4 #Distance between agent and information
sharp <- 5 #The speed at which probability of integration will drop to zero around the latitude of acceptance

#Max distance?
# x1 <- -1
# y1 <- -1
# 
# x2 <- 1
# y2 <- 1
#Distance function
# sqrt( ((x2-x1)^2)+((y2-y1)^2)) # = 2.828427

D^sharp/(d^sharp+D^sharp)

```

###RUNNING MODEL TIME
```{r}
set.seed(234)
#The first model will get some parameters which are copied to all models to reduce the stochastic nature of sampling. The parameters are: start location (x & y), sharing threshold and Group. They will be copied manually because they are coded into the GenPop function.
Agents_mod1 <- read_csv("Agents_mod1.csv")
#MODEL 1:
Lat_reject <- 0.5

Agents_mod1 <- GenPop(nPop = 500, nMemo = 20, Lat_accept = 0.3, Sharpness = 5, nGroups = 10)

#Where do the agents start the simulation (default (0,0))
Agents_mod1[,2] <- runif(nrow(Agents_mod1), -1, 1) #Random x coordinates
Agents_mod1[,3] <- runif(nrow(Agents_mod1), -1, 1) #Random y coordinates

#After generating random starting positions for the Agents, also make the first memory equal to that starting position, so the first memory they integrate doesn't have too much influence, and they "remember" where they started.
Agents_mod1[,10] <- Agents_mod1[,2] #memory 1x
Agents_mod1[,11] <- Agents_mod1[,3] #memory 1y

model1 <- ABM1(Agents = Agents_mod1, Iterations = 400, Plot_freq = 50, Bias = "None", Social_posting = FALSE)

#MODEL 2:
Lat_reject <- 1

Agents_mod2 <- GenPop(nPop = 500, nMemo = 20, Lat_accept = 0.5, Sharpness = 5, nGroups = 10)

#Where do the agents start the simulation (default (0,0))
Agents_mod2[,2] <- Agents_mod1[,2] #Random x coordinates (from model 1)
Agents_mod2[,3] <- Agents_mod1[,3] #Random y coordinates (from model 1)

Agents_mod2[,10] <- Agents_mod2[,2] #memory 1x
Agents_mod2[,11] <- Agents_mod2[,3] #memory 1y

Agents_mod2[,8] <- Agents_mod1[,8] #Share column (from model 1)

Agents_mod2[,9] <- Agents_mod1[,9] #Group column (from model 1)

model2 <- ABM1(Agents = Agents_mod2, Iterations = 400, Plot_freq = 50, Bias = "None", Social_posting = FALSE)

#MODEL 3:
Lat_reject <- 1.5

Agents_mod3 <- GenPop(nPop = 500, nMemo = 20, Lat_accept = 0.7, Sharpness = 5, nGroups = 10)

#Where do the agents start the simulation (default (0,0))
Agents_mod3[,2] <- Agents_mod1[,2] #Random x coordinates (from model 1)
Agents_mod3[,3] <- Agents_mod1[,3] #Random y coordinates (from model 1)

Agents_mod3[,10] <- Agents_mod3[,2] #memory 1x
Agents_mod3[,11] <- Agents_mod3[,3] #memory 1y

Agents_mod3[,8] <- Agents_mod1[,8] #Share column (from model 1)

Agents_mod3[,9] <- Agents_mod1[,9] #Group column (from model 1)

model3 <- ABM1(Agents = Agents_mod3, Iterations = 400, Plot_freq = 50, Bias = "None", Social_posting = FALSE)

#MODEL 4:
Lat_reject <- 2

Agents_mod4 <- GenPop(nPop = 500, nMemo = 20, Lat_accept = 1, Sharpness = 5, nGroups = 10)

#Where do the agents start the simulation (default (0,0))
Agents_mod4[,2] <- Agents_mod1[,2] #Random x coordinates (from model 1)
Agents_mod4[,3] <- Agents_mod1[,3] #Random y coordinates (from model 1)

Agents_mod4[,10] <- Agents_mod4[,2] #memory 1x
Agents_mod4[,11] <- Agents_mod4[,3] #memory 1y

Agents_mod4[,8] <- Agents_mod1[,8] #Share column (from model 1)

Agents_mod4[,9] <- Agents_mod1[,9] #Group column (from model 1)

model4 <- ABM1(Agents = Agents_mod4, Iterations = 400, Plot_freq = 50, Bias = "None", Social_posting = FALSE)

#MODEL 5:
Lat_reject <- 0.5

Agents_mod5 <- GenPop(nPop = 500, nMemo = 20, Lat_accept = 0.3, Sharpness = 5, nGroups = 10)

#Where do the agents start the simulation (default (0,0))
Agents_mod5[,2] <- Agents_mod1[,2] #Random x coordinates (from model 1)
Agents_mod5[,3] <- Agents_mod1[,3] #Random y coordinates (from model 1)

Agents_mod5[,10] <- Agents_mod5[,2] #memory 1x
Agents_mod5[,11] <- Agents_mod5[,3] #memory 1y

Agents_mod5[,8] <- Agents_mod1[,8] #Share column (from model 1)

Agents_mod5[,9] <- Agents_mod1[,9] #Group column (from model 1)

model5 <- ABM1(Agents = Agents_mod5, Iterations = 400, Plot_freq = 50, Bias = "None", Social_posting = TRUE)

#MODEL 6:
Lat_reject <- 1

Agents_mod6 <- GenPop(nPop = 500, nMemo = 20, Lat_accept = 0.5, Sharpness = 5, nGroups = 10)

#Where do the agents start the simulation (default (0,0))
Agents_mod6[,2] <- Agents_mod1[,2] #Random x coordinates (from model 1)
Agents_mod6[,3] <- Agents_mod1[,3] #Random y coordinates (from model 1)

Agents_mod6[,10] <- Agents_mod6[,2] #memory 1x
Agents_mod6[,11] <- Agents_mod6[,3] #memory 1y

Agents_mod6[,8] <- Agents_mod1[,8] #Share column (from model 1)

Agents_mod6[,9] <- Agents_mod1[,9] #Group column (from model 1)

model6 <- ABM1(Agents = Agents_mod6, Iterations = 400, Plot_freq = 50, Bias = "None", Social_posting = TRUE)

#MODEL 7:
Lat_reject <- 1.5

Agents_mod7 <- GenPop(nPop = 500, nMemo = 20, Lat_accept = 0.7, Sharpness = 5, nGroups = 10)

#Where do the agents start the simulation (default (0,0))
Agents_mod7[,2] <- Agents_mod1[,2] #Random x coordinates (from model 1)
Agents_mod7[,3] <- Agents_mod1[,3] #Random y coordinates (from model 1)

Agents_mod7[,10] <- Agents_mod7[,2] #memory 1x
Agents_mod7[,11] <- Agents_mod7[,3] #memory 1y

Agents_mod7[,8] <- Agents_mod1[,8] #Share column (from model 1)

Agents_mod7[,9] <- Agents_mod1[,9] #Group column (from model 1)

model7 <- ABM1(Agents = Agents_mod7, Iterations = 400, Plot_freq = 50, Bias = "None", Social_posting = TRUE)

#MODEL 8:
Lat_reject <- 2

Agents_mod8 <- GenPop(nPop = 500, nMemo = 20, Lat_accept = 1, Sharpness = 5, nGroups = 10)

#Where do the agents start the simulation (default (0,0))
Agents_mod8[,2] <- Agents_mod1[,2] #Random x coordinates (from model 1)
Agents_mod8[,3] <- Agents_mod1[,3] #Random y coordinates (from model 1)

Agents_mod8[,10] <- Agents_mod8[,2] #memory 1x
Agents_mod8[,11] <- Agents_mod8[,3] #memory 1y

Agents_mod8[,8] <- Agents_mod1[,8] #Share column (from model 1)

Agents_mod8[,9] <- Agents_mod1[,9] #Group column (from model 1)

model8 <- ABM1(Agents = Agents_mod8, Iterations = 400, Plot_freq = 50, Bias = "None", Social_posting = TRUE)

#write.csv(Agents_mod1, "C:\\Users\\cleme\\Desktop\\4. sem CogSci\\Computational Modelling\\CompMod-Exam\\Agents_mod1.csv", row.names = FALSE)

```


#Some more models with greater region of noncommitment
```{r}
#MODEL 9:
Lat_reject <- 2

Agents_mod9 <- GenPop(nPop = 500, nMemo = 20, Lat_accept = 0.3, Sharpness = 5, nGroups = 10)

#Where do the agents start the simulation (default (0,0))
Agents_mod9[,2] <- Agents_mod1[,2] #Random x coordinates (from model 1)
Agents_mod9[,3] <- Agents_mod1[,3] #Random y coordinates (from model 1)

Agents_mod9[,10] <- Agents_mod9[,2] #memory 1x
Agents_mod9[,11] <- Agents_mod9[,3] #memory 1y

Agents_mod9[,8] <- Agents_mod1[,8] #Share column (from model 1)

Agents_mod9[,9] <- Agents_mod1[,9] #Group column (from model 1)

model9 <- ABM1(Agents = Agents_mod9, Iterations = 400, Plot_freq = 50, Bias = "None", Social_posting = TRUE)

#MODEL 10:
Lat_reject <- 2

Agents_mod10 <- GenPop(nPop = 500, nMemo = 20, Lat_accept = 0.5, Sharpness = 5, nGroups = 10)

#Where do the agents start the simulation (default (0,0))
Agents_mod10[,2] <- Agents_mod1[,2] #Random x coordinates (from model 1)
Agents_mod10[,3] <- Agents_mod1[,3] #Random y coordinates (from model 1)

Agents_mod10[,10] <- Agents_mod10[,2] #memory 1x
Agents_mod10[,11] <- Agents_mod10[,3] #memory 1y

Agents_mod10[,8] <- Agents_mod1[,8] #Share column (from model 1)

Agents_mod10[,9] <- Agents_mod1[,9] #Group column (from model 1)

model10 <- ABM1(Agents = Agents_mod10, Iterations = 400, Plot_freq = 50, Bias = "None", Social_posting = TRUE)

#MODEL 11:
Lat_reject <- 2

Agents_mod11 <- GenPop(nPop = 500, nMemo = 20, Lat_accept = 0.3, Sharpness = 5, nGroups = 10)

#Where do the agents start the simulation (default (0,0))
Agents_mod11[,2] <- Agents_mod1[,2] #Random x coordinates (from model 1)
Agents_mod11[,3] <- Agents_mod1[,3] #Random y coordinates (from model 1)

Agents_mod11[,10] <- Agents_mod11[,2] #memory 1x
Agents_mod11[,11] <- Agents_mod11[,3] #memory 1y

Agents_mod11[,8] <- Agents_mod1[,8] #Share column (from model 1)

Agents_mod11[,9] <- Agents_mod1[,9] #Group column (from model 1)

model11 <- ABM1(Agents = Agents_mod11, Iterations = 400, Plot_freq = 50, Bias = "None", Social_posting = FALSE)

#MODEL 12:
Lat_reject <- 2

Agents_mod12 <- GenPop(nPop = 500, nMemo = 20, Lat_accept = 0.5, Sharpness = 5, nGroups = 10)

#Where do the agents start the simulation (default (0,0))
Agents_mod12[,2] <- Agents_mod1[,2] #Random x coordinates (from model 1)
Agents_mod12[,3] <- Agents_mod1[,3] #Random y coordinates (from model 1)

Agents_mod12[,10] <- Agents_mod12[,2] #memory 1x
Agents_mod12[,11] <- Agents_mod12[,3] #memory 1y

Agents_mod12[,8] <- Agents_mod1[,8] #Share column (from model 1)

Agents_mod12[,9] <- Agents_mod1[,9] #Group column (from model 1)

model12 <- ABM1(Agents = Agents_mod12, Iterations = 400, Plot_freq = 50, Bias = "None", Social_posting = FALSE)
```

#Save outputs
```{r}
#Writing csv of all model outputs so they are not lost

# write.csv(model1, "C:\\Users\\cleme\\Desktop\\4. sem CogSci\\Computational Modelling\\CompMod-Exam\\model1.csv", row.names = FALSE)
# 
# write.csv(model2, "C:\\Users\\cleme\\Desktop\\4. sem CogSci\\Computational Modelling\\CompMod-Exam\\model2.csv", row.names = FALSE)
# 
# write.csv(model3, "C:\\Users\\cleme\\Desktop\\4. sem CogSci\\Computational Modelling\\CompMod-Exam\\model3.csv", row.names = FALSE)
# 
# write.csv(model4, "C:\\Users\\cleme\\Desktop\\4. sem CogSci\\Computational Modelling\\CompMod-Exam\\model4.csv", row.names = FALSE)
# 
# write.csv(model5, "C:\\Users\\cleme\\Desktop\\4. sem CogSci\\Computational Modelling\\CompMod-Exam\\model5.csv", row.names = FALSE)
# 
# write.csv(model6, "C:\\Users\\cleme\\Desktop\\4. sem CogSci\\Computational Modelling\\CompMod-Exam\\model6.csv", row.names = FALSE)
# 
# write.csv(model7, "C:\\Users\\cleme\\Desktop\\4. sem CogSci\\Computational Modelling\\CompMod-Exam\\model7.csv", row.names = FALSE)
# 
# write.csv(model8, "C:\\Users\\cleme\\Desktop\\4. sem CogSci\\Computational Modelling\\CompMod-Exam\\model8.csv", row.names = FALSE)
#
# write.csv(model9, "C:\\Users\\cleme\\Desktop\\4. sem CogSci\\Computational Modelling\\CompMod-Exam\\model9.csv", row.names = FALSE)
# 
# write.csv(model10, "C:\\Users\\cleme\\Desktop\\4. sem CogSci\\Computational Modelling\\CompMod-Exam\\model10.csv", row.names = FALSE)
#
# write.csv(model11, "C:\\Users\\cleme\\Desktop\\4. sem CogSci\\Computational Modelling\\CompMod-Exam\\model11.csv", row.names = FALSE)
# 
# write.csv(model12, "C:\\Users\\cleme\\Desktop\\4. sem CogSci\\Computational Modelling\\CompMod-Exam\\model12.csv", row.names = FALSE)
```

#Combining outputs
```{r}

model1 <- read_csv("model1.csv")
model2 <- read_csv("model2.csv")
model3 <- read_csv("model3.csv")
model4 <- read_csv("model4.csv")
model5 <- read_csv("model5.csv")
model6 <- read_csv("model6.csv")
model7 <- read_csv("model7.csv")
model8 <- read_csv("model8.csv")
model9 <- read_csv("model9.csv")
model10 <- read_csv("model10.csv")
model11 <- read_csv("model11.csv")
model12 <- read_csv("model12.csv")


model1$Social_posting <- 0
model2$Social_posting <- 0
model3$Social_posting <- 0
model4$Social_posting <- 0
model5$Social_posting <- 1
model6$Social_posting <- 1
model7$Social_posting <- 1
model8$Social_posting <- 1
model9$Social_posting <- 1
model10$Social_posting <- 1
model11$Social_posting <- 0
model12$Social_posting <- 0

model1$model_no <- 1
model2$model_no <- 2
model3$model_no <- 3
model4$model_no <- 4
model5$model_no <- 5
model6$model_no <- 6
model7$model_no <- 7
model8$model_no <- 8
model9$model_no <- 9
model10$model_no <- 10
model11$model_no <- 11
model12$model_no <- 12

model1$LoR <- 0.5
model2$LoR <- 1
model3$LoR <- 1.5
model4$LoR <- 2
model5$LoR <- 0.5
model6$LoR <- 1
model7$LoR <- 1.5
model8$LoR <- 2
model9$LoR <- 2
model10$LoR <- 2
model11$LoR <- 2
model12$LoR <- 2

model1$clusters <- 1
model2$clusters <- 1
model3$clusters <- 1
model4$clusters <- 2
model5$clusters <- 1
model6$clusters <- 1
model7$clusters <- 1
model8$clusters <- 2
model9$clusters <- 5
model10$clusters <- 3
model11$clusters <- 4
model12$clusters <- 3


df <- rbind(model1, model2, model3, model4, model5, model6, model7, model8, model9, model10, model11, model12)
df$Social_posting <- as.factor(df$Social_posting)
df$model_no <- as.factor(df$model_no)
df$AgentNo <- as.factor(df$AgentNo)
df$clusters <- as.integer(df$clusters)
str(df)
```


#Investigative analyses
```{r}
df %>%
    ggplot(aes(x, y, color = Group)) + geom_point(aes(size = Share, alpha = 0.3)) + 
  xlim(-1, 1) +
  ylim(-1, 1) +  
  ggtitle("Output of last iteration") +
  scale_size(trans = 'reverse') +
  facet_wrap(~model_no, labeller = "label_both")


df %>% 
  ggplot(aes(No_of_Shares, color = Group)) + geom_density()


df %>% 
  ggplot(aes(Share, color = Group)) + geom_density()

df %>%
  count(Group, sort = T) %>%  #Group size + density might affect radicalization.
  mutate(group_density = n/sum(n))
  
#Difference between groups
df %>%  #For explorative analyses
  group_by(Group) %>% 
  summarise(mean_no_share = mean(No_of_Shares),
            sd_no_share = sd(No_of_Shares),
            mean_valence = mean(Valence),
            sd_valence = sd(Valence))

#Difference between models
df %>% 
  group_by(model_no) %>% 
  summarise(mean_no_share = mean(No_of_Shares),
            sd_no_share = sd(No_of_Shares),
            mean_valence = mean(Valence),
            sd_valence = sd(Valence))


#Difference between groups (the next 3 plots: x, y, valence)
df %>% 
  group_by(Group) %>% 
  summarise(mean_x = mean(x)) %>% 
  ggplot(aes(x = Group, y = mean_x, fill = Group)) +
  geom_bar(stat = "identity") +
    labs(
        x = "Group",
        y = "Mean X",
        title = paste(
            "Mean X by group"
        )
    )

df %>% 
  group_by(Group) %>% 
  summarise(mean_y = mean(y)) %>% 
  ggplot(aes(x = Group, y = mean_y, fill = Group)) +
  geom_bar(stat = "identity") + 
    theme_classic() +
    labs(
        x = "Group",
        y = "Mean Y",
        title = paste(
            "Mean Y by group"
        )
    )

df %>% 
  group_by(Group) %>% 
  summarise(mean_valence = mean(Valence)) %>% 
  ggplot(aes(x = Group, y = mean_valence, fill = Group)) +
  geom_bar(stat = "identity") + 
    theme_classic() +
    labs(
        x = "Group",
        y = "Mean Valence",
        title = paste(
            "Mean valence by group"
        )
    )


#Difference between models (the next three plots: x, y, valence)
df %>% 
  group_by(model_no) %>% 
  summarise(mean_x = mean(x),
            sd_x = sd(x)) %>% 
  ggplot(aes(x = model_no, y = mean_x, fill = model_no)) +
  geom_bar(stat = "identity") + 
  geom_errorbar(mapping = aes(ymin = (mean_x -sd_x),
                              ymax = (mean_x+sd_x)), size =0.3, width = 0.3) +
    theme_classic() +
    labs(
        x = "Model No",
        y = "Mean X",
        title = paste(
            "Mean X by model No"
        )
    )


df %>% 
  ggplot() + geom_density(aes(x = x), color = "red") + geom_density(aes(x = y), color = "blue") + facet_wrap(~model_no, labeller = "label_both", ncol = 3, nrow = 4) +
  ggtitle("Dispersion of agents across both axes",
          subtitle = "x = Red, y = Blue")

df %>% 
  group_by(model_no) %>% 
  summarise(mean_y = mean(y)) %>% 
  ggplot(aes(x = model_no, y = mean_y, fill = model_no)) +
  geom_bar(stat = "identity") + 
    theme_classic() +
    labs(
        x = "Model No",
        y = "Mean Y",
        title = paste(
            "Mean Y by model No"
        )
    )

df %>% 
  group_by(model_no) %>% 
  summarise(mean_valence = mean(Valence)) %>% 
  ggplot(aes(x = model_no, y = mean_valence, fill = model_no)) +
  geom_bar(stat = "identity") + 
    theme_classic() +
    labs(
        x = "Model No",
        y = "Mean Valence",
        title = paste(
            "Mean valence by Model No"
        )
    ) #it looks like lat of rejection predicts overall valence which makes sense.

#Looking further at LoR
df %>% 
  group_by(LoR) %>% 
  summarise(mean_valence = mean(Valence)) %>% 
  ggplot(aes(x = LoR, y = mean_valence, fill = LoR)) +
  geom_bar(stat = "identity") +
    labs(
        x = "Latitude of rejection",
        y = "Mean Valence",
        title = paste(
            "Mean valence by LoR"
        )
    ) #LoR is definitely a predictor of valence

df %>% 
  group_by(Lat) %>% 
  summarise(mean_valence = mean(Valence)) %>% 
  ggplot(aes(x = Lat, y = mean_valence, fill = Lat)) +
  geom_bar(stat = "identity") + 
    theme_classic() +
    labs(
        x = "Lat. of acceptance",
        y = "Mean Valence",
        title = paste(
            "Mean valence by Latitude of acceptance"
        )
    )

#How about modelling the distance between LoA and LoR: the region of indifference
df$model_no <- ordered(df$model_no, levels = c(1:8, "10", "12", "9", "11"))
df %>%
  group_by(model_no) %>% 
  summarise(mean_valence = mean(Valence),
            region_indiff = factor(LoR-Lat), levels = c(region_indiff)) %>% 
  ggplot(aes(x = region_indiff, y = mean_valence, color = model_no)) +
  geom_point() +
  geom_text(aes(label = model_no, color = model_no), size = 4, position=position_dodge(width = 0.5)) + 
  scale_x_discrete() +
  theme(legend.position = "None") +
    labs(
        x = "Region of noncommitment",
        y = "Mean Valence",
        title = paste(
            "Mean valence by region of noncommitment"
        ),
        subtitle = "Grouped by model_no"
    ) #Great! Notice the models that have the same RoN still have a small difference in valence, depending on whether social media is turned on or not. Also notice model 9 and 11 stacked on each other. They have the same mean valence, but a different number of clusters: interesting!

#Looking at the effect of social posting
n <- 6000
xbar <- mean(df$Valence)
s <- sd(df$Valence)
margin <- qt(0.975,df=n-1)*s/sqrt(n)

df %>% 
  group_by(Social_posting) %>% 
  summarise(mean_valence = mean(Valence),
            sd_valence = sd(Valence)) %>% 
  ggplot(aes(x = Social_posting, y = mean_valence, fill = Social_posting)) +
  geom_bar(aes(width = 0.5), stat = "identity") + 
  geom_errorbar(mapping = aes(ymin = (mean_valence - margin),
                              ymax = (mean_valence + margin)), size =0.3, width = 0.3) +
    labs(
        x = "Social Posting",
        y = "Mean Valence",
        title = paste(
            "Mean valence by Social Posting (CI error)"
        )
    )

df %>% 
  ggplot(aes(Valence, color = Group)) + geom_boxplot()

df %>% 
  ggplot(aes(x, color = model_no)) + geom_boxplot()

df %>% 
  ggplot(aes(y, color = model_no)) + geom_boxplot()

df %>% 
  ggplot(aes(x)) + geom_density(aes(color = model_no)) + ggtitle("Clustering of agents across X")

df %>% 
  ggplot(aes(y)) + geom_density(aes(color = model_no)) + ggtitle("Clustering of agents across Y")

df %>% 
  ggplot(aes(x)) + geom_density(aes(color = Social_posting)) + ggtitle("Clustering of agents across X")

df %>% 
  ggplot(aes(y)) + geom_density(aes(color = Social_posting)) + ggtitle("Clustering of agents across Y")
  
```
 
#Spatial cluster analysis
```{r}
points <- 
  df %>%
  filter(model_no == 1) %>% 
  select(x, y)

kclust <- kmeans(points, centers = 5)

summary(kclust)

tidy(kclust)
augment(kclust, points)
glance(kclust)

#Let’s say we want to explore the effect of different choices of k, from 1 to 9, on this clustering. First cluster the data 9 times, each using a different value of k, then create columns containing the tidied, glanced and augmented data:
kclusts <- 
  tibble(k = 1:9) %>%
  mutate(
    kclust = map(k, ~kmeans(points, .x)),
    tidied = map(kclust, tidy),
    glanced = map(kclust, glance),
    augmented = map(kclust, augment, points)
  )


#Now unnest the data into seperate dataframes. This way we can compare and see how many clusters we optimally should have

clusters <- 
  kclusts %>%
  unnest(cols = c(tidied))

assignments <- 
  kclusts %>% 
  unnest(cols = c(augmented))

clusterings <- 
  kclusts %>%
  unnest(cols = c(glanced))

p1 <- 
  ggplot(assignments, aes(x = x, y = y)) +
  geom_point(aes(color = .cluster), alpha = 0.8) + xlim(-1, 1) + ylim(-1, 1) +
  facet_wrap(~ k)
p1

#add "X" to make clear where kmeans places the clusters
p2 <- p1 + geom_point(data = clusters, size = 10, shape = "x")
p2

#The data from glance() fills a different but equally important purpose; it lets us view trends of some summary statistics across values of k. Of particular interest is the total within sum of squares, saved in the tot.withinss column.
ggplot(clusterings, aes(k, tot.withinss)) +
  geom_line() +
  geom_point()

#This represents the variance within the clusters. It decreases as k increases, but notice a bend (or “elbow”) around k = 4. This bend indicates that additional clusters beyond the third have little value. (See here for a more mathematically rigorous interpretation and implementation of this method). Thus, all three methods of tidying data provided by broom are useful for summarizing clustering output.

#Source: https://www.tidymodels.org/learn/statistics/k-means/
```

#Spacial analysis into function
```{r}
spatial_analysis <- function(model_num, df){
  
  #Specify points to work with
  points <- 
  df %>%
  filter(model_no == model_num) %>% 
  select(x, y)
  
  #loop through potential number of cluseters (k)
  kclusts <- 
  tibble(k = 1:9) %>%
  mutate(
    kclust = map(k, ~kmeans(points, .x)),
    tidied = map(kclust, tidy),
    glanced = map(kclust, glance),
    augmented = map(kclust, augment, points)
  )
  
  #Now unnest the data into seperate dataframes. This way we can compare and see how many clusters we optimally should have
  clusters <- 
    kclusts %>%
    unnest(cols = c(tidied))
  
  assignments <- 
    kclusts %>% 
    unnest(cols = c(augmented))
  
  clusterings <- 
    kclusts %>%
    unnest(cols = c(glanced))
  
  #Plot
  p1 <- 
  ggplot(assignments, aes(x = x, y = y)) +
  geom_point(aes(color = .cluster), alpha = 0.8) + xlim(-1, 1) + ylim(-1, 1) +
    ggtitle(paste("Model number", model_num)) +
    theme(legend.position = "None") +
  facet_wrap(~ k, labeller = "label_both")
  
  p2 <- p1 + geom_point(data = clusters, size = 10, shape = "x")
  p2
    
  #Find optimal number of clusters 
 cluster_plot <-  ggplot(clusterings, aes(k, tot.withinss)) +
  geom_line() +
  geom_point() +
   ggtitle(paste("Model number", model_num))
 
 print(cluster_plot)
 return(p2)
 
 #return(cluster_plot)
 
}


clusters_mod1 <- spatial_analysis(1, df)
clusters_mod2 <- spatial_analysis(2, df)
clusters_mod3 <- spatial_analysis(3, df)
clusters_mod4 <- spatial_analysis(4, df)
clusters_mod5 <- spatial_analysis(5, df)
clusters_mod6 <- spatial_analysis(6, df)
clusters_mod7 <- spatial_analysis(7, df)
clusters_mod8 <- spatial_analysis(8, df)
clusters_mod9 <- spatial_analysis(9, df)
clusters_mod10 <- spatial_analysis(10, df)
clusters_mod11 <- spatial_analysis(11, df)
clusters_mod12 <- spatial_analysis(12, df)

clusters_mod1 #1 cluster
clusters_mod2 #1 cluster
clusters_mod3 #1 cluster
clusters_mod4 #2 cluster
clusters_mod5 #1 cluster
clusters_mod6 #1 cluster
clusters_mod7 #1 cluster
clusters_mod8 #2 cluster
clusters_mod9 #5 clusters
clusters_mod10 #3 clusters
clusters_mod11 #4 clusters
clusters_mod12 #3 clusters
```

  
#Bayesian analysis
```{r}
#This section is a trainwreck. Apologize, but I didn't have time to figure it out. The problem was a distribution of the outcome "valence". Besides the plots told a pretty clear story. 

str(df)
#Let's try to model valence as outcome
df %>% 
  ggplot(aes(Valence)) + geom_density(aes(color = Social_posting)) #It's almost lognormal

df %>% 
  mutate(Valence_log = log(Valence)) %>% 
  ggplot(aes(Valence_log)) + geom_density() #I'm not sure what to do with a distribution like this.

df %>% 
  group_by(Social_posting) %>% 
  summarise(mean_valence = mean(Valence),
            sd_valence = sd(Valence))

f1 <- bf(Valence ~ Social_posting + Lat + (1 | AgentNo))

get_prior(f1, data = df, family = lognormal())

f1_prior <- c(
  prior(normal(0, 0.1), class = b, coef = Social_posting1),# No diff
  prior(normal(0, 0.2), class = b, coef = Lat), #No difference but less conservative
  prior(student_t(3, 0, 2.5), class = sd), #Default
  prior(student_t(3, 0, 2.5), class = sigma) #Default
)

m1_prior <- brm(
  f1,
  data = df,
  family = gaussian(),
  prior = f1_prior,
  sample_prior = "only",
  chains = 2,
  cores = 4,
  backend="cmdstanr",
  threads = threading(2),
  )

pp_check(m1_prior, nsamples = 50)


m1_post <- brm(
  f1,
  data = df,
  family = gaussian(),
  prior = f1_prior,
  sample_prior = T,
  chains = 2,
  cores = 4,
  backend="cmdstanr",
  threads = threading(2),
  )

pp_check(m1_post, nsamples = 50)

conditional_effects(m1_post)

posterior_gauss <- posterior_samples(m1_post)
summary(m1_post)
colnames(posterior_gauss)

ggplot(posterior_gauss) + #Plotting the beta prior + posterior
  theme_classic() +
  geom_density(aes(prior_b), fill="red", alpha=0.3) +
  geom_density(aes(b_Social_posting1), fill="blue", alpha=0.5)

ggplot(posterior_gauss) + #Plotting the sigma prior + posterior
  theme_classic() +
  geom_density(aes(prior_sigma), fill="red", alpha=0.3) +
  geom_density(aes(sigma), fill="blue", alpha=0.5)

##SAMPLING QUALITY
mcmc_trace(m1_post, 
           pars = c("b_Social_posting1", 
                    "sd_AgentNo__Intercept",
                    'sigma')) +
  theme_classic() 

mcmc_rank_overlay(m1_post,
                  pars = c("b_Social_posting1", 
                    "sd_AgentNo__Intercept",
                    'sigma')) + 
  theme_classic()


```

