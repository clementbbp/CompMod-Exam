---
title: "Information Integration Project"
author: "Clement Peters"
date: "5/1/2021"
output: html_document
---

#What do I need to do:

1. Define Agents
  1.1 What types. Agents + info-bits
  1.2 What parameters do they have
    1.2.1 *Agents*
        Memory
            How large is memory? Parameter*
        Location in attitude space (xy coordinate)
            This is the mean of the attitudes they hold in memory
            I can make a parameter specifying how closely to the middle the population               will start
    1.2.2 *Info bit*
        Lifespan
            Should be one tick. If they are not integrated they disappear
        Location in attitude space
            Should be random, with a distribution of -1 to 1 on both axis. This will
            represent central information output. Could also be programmed with a                   certain bias, which might represent central information better.
        Trustworthiness score
            How trustworthy is the information perceived generally?
  1.3 What will they do each tick?
    1.3.1 Agents will try to integrate the infobit which is created. Success will depend
        on the information integration formula which gives a probability of being inte-
        grated. *If the info-bit is integrated, they will move closer to that info-bit in         attitude space by averaging and updating their memory*
    1.3.2 If social media is turned on, then the agents will have a probability to share
          their location in attitude space for others to integrate
    1.3.3 Info bits will disappear after one tick
    1.3.4
        
      
 
```{r}
##Packages
pacman::p_load(tidyverse, stringr, dplyr, brms, bayesplot,
               loo,
               lme4)
```

#Functions for use later: 
```{r}
Integrate_main <- function(Agents, i, Info){
  
  # #If true, check if memory is full
         for(j in 10:ncol(Agents)){

        #Check if memory slot is open
         if(Agents[i,j] == 0){
           j <- as.numeric(j)

           #If true, save the information to the empty memory space
           Agents[i,j] <- Info$xi[1]
           Agents[i,(j+1)] <- Info$yi[1]

            #Break out of loop and continue to next agent
            break
         }
           
        }
  
        #If full, sample memory space, and replace with new value (integrate)
        
        #which place in memory should the new info go
        
        #Sample which memory should be replaced. There are nMemo possible values
        sample_num <- as.character(sample(1:((ncol(Agents)-9)/2), size=1)) 
        
        #Now i have the location (sample_num), I just need to plot in the new                   information
        colnames_df <- colnames(Agents[i,])
       
        memory_to_replace <- as.data.frame(str_detect(colnames_df, sample_num))
        
        #Now I know the columns in which the new info should go. So I need to put it there
        column <- numeric()
        for(k in 1:nrow(memory_to_replace)){
          if(memory_to_replace[k,] == TRUE){
            k <- as.numeric(k)
            column <- c(column, k) # Saving the index of the columns
            #column <- as.list(column)
            
          }
        }
        x_coord <- column[[1]] # memory x coordinate column number
        y_coord <- column[[2]] # memory y coordinate column number
        
        # And now we save the new memory in the right location
        Agents[i,][,x_coord] <- Info$xi[1]
        Agents[i,][,y_coord] <- Info$yi[1]
        
        #Now memory is updated. Time to update location on attitude space
        
        #First reset agent position to 0 before updating memory
        Agents$x[i] <- 0
        Agents$y[i] <- 0
        
        #Because the location of the agent is dependent on its memory, it will always be zero to begin with because almost all memories are "neutral". I need to average only over the non-zero memories.
        
        #Number of non-zero columns per agent a.k.a. how many memory spaces are filled + other variables
        
         nonZero <- sum(apply(Agents[i,], 2, function(c)sum(c!=0)))
         nonZero <- as.numeric(nonZero)
        
        # Averaging the x-values over the number of memories (of value x)
        Agents$x[i] <-  (sum(Agents[i,seq(10, ncol(Agents), 2)])/((nonZero-7)/2)) # -9 because there are always those 9 non memory columns which are nonZero, assuming that x and y are non zero.
        
        # Averaging the y-values over the number of memories (of value y)
        Agents$y[i] <- (sum(Agents[i,seq(11, ncol(Agents), 2)])/((nonZero-7)/2)) 
       return(Agents)
      }

Integrate_social <- function(Agents, Social_group, s, Info){
  
  # #If true, check if memory is full
         for(v in 10:ncol(Social_group)){
        
        #Check if memory slot is open
         if(Social_group[s,v] == 0){
        
           #If true, save the information to the empty memory space
           Social_group[s,v] <- Info$xi[2]
           Social_group[s,(v+1)] <- Info$yi[2]
           
           #Break out of loop and continue to next agent
           break
         }
        }
  
        #If full, sample memory space, and replace with new value (integrate)
        
        #which place in memory should the new info go
        
        #Sample which memory should be replaced. There are nMemo possible values
        sample_num <- as.character(sample(1:((ncol(Social_group)-9)/2), size=1))
        
        #Now i have the location (sample_num), I just need to plot in the new                   information
        colnames_df <- colnames(Social_group[s,])
       
        memory_to_replace <- as.data.frame(str_detect(colnames_df, sample_num))
        
        #Now I know the columns in which the new info should go. So I need to put it there
        column_social <- numeric()
        for(m in 1:nrow(memory_to_replace)){ #Two rows to loop through: memory x and y
          if(memory_to_replace[m,] == TRUE){
            
            column_social <- c(column_social, m) # Saving the index of the columns
            column_social <- as.list(column_social)
            
          }
        }
        x_coord <- column_social[[1]] # memory x coordinate column number
        y_coord <- column_social[[2]] # memory y coordinate column number
        
        #Reset agent position to 0 before updating memory
        Social_group$x[s] <- 0
        Social_group$y[s] <- 0
        
        # And now we save the new memory in the right location
        Social_group[s,][,x_coord] <- Info$xi[2]
        Social_group[s,][,y_coord] <- Info$yi[2]
        
        #Now memory is updated. Time to update location on attitude space
        
        #Because the location of the agent is dependent on its memory, it will always be zero to begin with because almost all memories are "neutral". I need to average only over the non-zero memories.
        
        #Number of non-zero columns per agent
        nonZero <- sum(apply(Social_group[s,], 2, function(c)sum(c!=0)))
        
        # Averaging the x-values over the number of memories (of value x)
        Social_group$x[s] <- sum(Social_group[s,seq(10, ncol(Social_group), 2)])/((nonZero-7)/2) # -7 because there are always those 7 non memory columns which are nonZero (at this place in the loop x and y are always zero)
        
        # Averaging the y-values over the number of memories (of value y)
        Social_group$y[s] <- sum(Social_group[s,seq(11, ncol(Social_group), 2)])/((nonZero-7)/2) 
        
        return(Social_group)
        } #Done integrating
```        
  
#d=√((x2−x1)^2+(y2−y1)^2) the distance between two points
# f(d;D,δ) = D^δ/(d^δ+D^δ) Integration formula
  
```{r}
D <- 0.4 #What should the max latitude be when the maximum distance in the attitude space can only be 2.83. It is realistic that a piece of information has 0.5 chance of being integrated, when it's theoretically as far away from the agent as possible? 

DR <- 1 #Latitude of rejection
d <- 1.1 #1/4 of max distance
sharp <- 2
#Max distance?
# x1 <- -1
# y1 <- -1
# 
# x2 <- 1
# y2 <- 1
# sqrt( ((x2-x1)^2)+((y2-y1)^2)) # = 2.828427

D^sharp/(d^sharp+D^sharp)

DR^sharp/(d^sharp+DR^sharp)

#Define Agents
GenPop <- function(nPop, nMemo, Lat_accept, Sharpness, nGroups){
  

Agent <- data.frame(AgentNo = 1:nPop, # agent number
                    x = 0, # y coordinate
                    y = 0, # x coordinate
                    Lat = Lat_accept, #latitude of acceptance
                    Sharpness = Sharpness,
                    Valence = 1,
                    No_of_Shares = 1,
                    stringsAsFactors = FALSE)

#This column can't be zero because I use it for calculations later
Agent$No_of_Shares <- as.numeric(Agent$No_of_Shares)

#Assigning each agent to a group
groups <- as.data.frame(matrix(0, nrow = nrow(Agent), ncol = 1))
groups <- rename(groups, Group = V1)

for(a in 1:nrow(groups)){
  
  #It's debatable whether there should be an equal amount of members in each group, or if it should be random. I'm going to start with random amounts (but roughly equal), but later I might change in to be able to control group dynamics.
  groups[a,] <- sample(LETTERS[1:nGroups], 1, replace = TRUE)
}

#Deciding a "sharing threshold" for each agent
sharing <- as.data.frame(matrix(0, nrow = nrow(Agent), ncol = 1))
sharing <- rename(sharing, Share = V1)

for(a in 1:nrow(groups)){
  
  #Each agent will have a value indicating how likely they are to share their point of     view to other agents of their group. I assume most people will not be very likely to share, while a few will likely always share.
  sharing[a,] <- runif(1,0,2) # OR rexp(1, 2)
  
  
}

#Building memory as dataframe with nMemo*2 column (x & y coordinates)
memory <- as.data.frame(matrix(0, nrow = nrow(Agent), ncol = nMemo*2))

prefixX <- "memory x"
prefixY <- "memory y"
suffix <- seq(1:nMemo)

namesx <- paste(prefixX, suffix, sep = "")
namesy <- paste(prefixY, suffix, sep = "")


colnames(memory)[seq(1,ncol(memory),2)] <- c(namesx)
colnames(memory)[seq(2,ncol(memory),2)] <- c(namesy)

Agent <- cbind(Agent, sharing, groups, memory)

return(Agent) 
  }

 
str(Agents)

#range01 <- function(x){(x-min(x))/(max(x)-min(x))}
range01 <- function(x){(x-0.0001)/(2.828427-0.0001)}

range01()
#I need a function which flips the direction of the infobit, but when the distance is small, its effect should be heavily reduced, but when distance is far, effect should be bigger. 


Agents <- GenPop(nPop = 100, nMemo = 10, Lat_accept = 0.3, Sharpness = 2, nGroups = 5)

Agents %>% 
  ggplot(aes(Share)) + geom_density()
  
Agents %>% 
  count(Group, sort = T)

Info <- data.frame(Info_num = 1:2, # Two pieces of into: main and social
                   xi = 0.5, # x coordinate for info
                   yi = 0.3, # y coordinate for info
                   Trust = 0, # Trustworthiness score Parameter*
                   TimeExist = 0)

#Where do the agents start the simulation (default (0,0))
Agents[,2] <- runif(nrow(Agents), -1, 1) #Random x coordinates
Agents[,3] <- runif(nrow(Agents), -1, 1) #Random y coordinates


ABM1 <- function(Agents, Iterations, Plot_freq, Bias, Social_posting){

  #Loop through time
  for(t in 1:Iterations){
    
    #You could argue that the (mainstream) information should have a biased distribution      since the attitudinal space represents a certain topic, and mainstream media is         unlikely to represent it in a completely random manner.
    if(Bias == "None"){
    
    #Creating neutral information
    Info$xi[1] <- runif(1, -1, 1) #No bias
    Info$yi[1] <- runif(1, -1, 1) #No bias
    
    }
  
   #Conditional statement for Bias
    if(Bias == "Positive"){
    
    
    #Creating biased information
    Info$xi[1] <- rnorm(1, 0.5, 1) #positive bias
    Info$yi[1] <- rnorm(1, 0.5, 1) #Positive bias
    
    #keeping the values between -1 and 1
    while(Info$xi[1] < -1 | Info$xi[1] > 1 | Info$yi[1] < -1 | Info$yi[1] > 1){ 
    Info$xi[1] <- rnorm(1, 0.5, 1) #positive bias
    Info$yi[1] <- rnorm(1, 0.5, 1) #Positive bias
    
    }
  }
    
    if(Bias == "Negative"){
    
    #Creating biased information
    Info$xi[1] <- rnorm(1, -0.5, 1) #negative bias
    Info$yi[1] <- rnorm(1, -0.5, 1) #negative bias
    
    #keeping the values between -1 and 1
    while(Info$xi[1] < -1 | Info$xi[1] > 1 | Info$yi[1] < -1 | Info$yi[1] > 1){ 
    Info$xi[1] <- rnorm(1, -0.5, 1) #negative bias
    Info$yi[1] <- rnorm(1, -0.5, 1) #negative bias
    
    }
  }
  
    #Loop through population
    for(i in sample(c(1:nrow(Agents)))){ #loop in random order
      
      #Find the acceptance latitude of agent
      Latitude_m <- Agents$Lat[i]
      
      #Find attitude sharpness of agent
      Sharpness_m <- Agents$Sharpness[i]
      
      # Present information to agent:
      
      #which information will the agent meet
      Info_meet <- Info[1,]
      
      #What is the distance between agent and info
      Distance_main <- sqrt( ((Agents$x[i] - Info$xi[1])^2) + ((Agents$y[i] -
                                                                  Info$yi[1])^2) )
      
      #What is the trustworthiness of info
      Trust <- Info$Trust[1]
      
      #What is the chance they they will integrate
      RandNum <- runif(1,0,1)
      if (RandNum < ((Latitude_m^Sharpness_m)/
                     (Distance_main^Sharpness_m+Latitude_m^Sharpness_m))){
        
        Agents <- Integrate_main(Agents = Agents,
                                 i = i,
                                 Info = Info) 
        
      } else{ #if information is not integrated normally, and if the distance is higher than the latitude of rejection, the attitudinal message is reversed, and then integrated, causing a contrast effect. This might only be relevant for social sharing.
        
        if(Distance_main > Lat_reject){
          
          #Flip the direction of the attitudinal message: contrast effect
            if (Info$xi[1] >= 0){
            
            #pull valence in opposite direction
            Info$xi[1] <- (Info$xi[1]) - ((range01(Distance_main))*2) 
            
          } else{ #If Info$xi[1] <= 0 (negative)
            
            #Pull valence in opposite direction
            Info$xi[1] <- (Info$xi[1]) - (-(range01(Distance_main))*2)
          }
          
          if (Info$yi[1] >= 0){
            
            #pull valence in opposite direction
            
            Info$yi[1] <- (Info$yi[1]) - ((range01(Distance_main))*2) 
            
          } else{ #If Info$yi[1] <= 0 (negative)
            
            #Pull valence in opposite direction
            Info$yi[1] <- (Info$yi[1]) - (-(range01(Distance_main))*2)
          } #Done implementing boomerang effect
        
          #Integrate new and reversed information:
          Agents <- Integrate_main(Agents = Agents, 
                                   i = i, 
                                   Info = Info)
        }
      } #Done integrating mainstream information 
      
     #Determine the absolute valency of agent "opinion" 
      Agents$Valence[i] <- sqrt((Agents$x[i])^2 + (Agents$y[i])^2)
      
      #Turn Social_posting on/off
      if(Social_posting == TRUE){
      
      #For every agent, see if they reach over the sharing threshold. Agents with high sharing value will have an easier time breaching the threshold  and will share more often.
        
      if(Agents$Valence[i] > Agents$Share[i]){
        
        #Index forward No_of_Shares
        Agents$No_of_Shares[i] <-  Agents$No_of_Shares[i] + 1
        
        #If true, the agent will "post" it's location to members of it's group
        Info$xi[2] <- Agents$x[i]
        Info$yi[2] <- Agents$y[i]
        
        #Grab subset of agents who are in the same group as agent "i"
        #Find a way to have each person in social group integrate agent i's information.
        Social_group <- subset(Agents, Agents$Group == Agents$Group[i])
        
        #Removing agent "i" from subset, so she doesn't integrate her own information
        Social_group <- Social_group %>% 
          filter(AgentNo != i)
        
        #Loop through each agent in social group as they try to integrate the information
        for(s in sample(c(1:nrow(Social_group)))){ #Loop in random order
          
          #Find the acceptance latitude of agent
          Latitude_social <- Social_group$Lat[s]
      
          #Find attitude sharpness of agent
          Sharpness_social <- Social_group$Sharpness[s]
          
          #distance between agent and social post
          Distance_social <- sqrt( ((Social_group$x[s] - Info$xi[2])^2) + 
                                     ((Social_group$y[s] -   Info$yi[2])^2) )
          
           #Each agent tries to integrate
          #What is the chance they they will integrate
        RandNum_social <- runif(1,0,1)
        if (RandNum_social <
            Latitude_social^Sharpness_social/(Distance_social^Sharpness_social+Latitude_social^Sharpness_social)){
        
        #if true, integrate
        Social_group <- Integrate_social(Agents = Agents, 
                                         Social_group = Social_group, 
                                         s = s,
                                         Info = Info)
       
      #Merge function doesn't work. But here I replace updated information (from Group) to original dataframe
      Agent_names <- Social_group$AgentNo
    for(x in Agent_names){
      
      #Check where the agent is in the original dataframe (AgentNo in Agents)
      if(Social_group$AgentNo[x] %in% Agents$AgentNo){
        
        #If true, replace old data new updated data
        Agents[Agents$AgentNo == x,] <- Social_group[Social_group$AgentNo == x,]
      
          }
        } #Done updating (merging) dataframe
      } else { #Adding boomerang effect
        
        if(Distance_social > Lat_reject_social){
          
          if (Info$xi[2] >= 0){
            Distance_social <- 0.5
            #pull valence in opposite direction
            Info$xi[2] <- (Info$xi[2]) - ((range01(Distance_social))*2)
            
          } else{ #If Info$xi[2] <= 0 (negative)
            
            #Pull valence in opposite direction
            Info$xi[2] <- (Info$xi[2]) - (-(range01(Distance_social))*2)
          }
          
        # "Flip" y
          if (Info$yi[2] >= 0){
            
            #pull valence in opposite direction
            Info$yi[2] <- (Info$yi[2]) - ((range01(Distance_social))*2) 
            
          } else{ #If Info$xi[1] <= 0 (negative)
            
            Info$yi[2] <- (Info$yi[2]) - (-(range01(Distance_social))*2)
          } #Done implementing boomerang effect
          
          Social_group <- Integrate_social(Agents = Agents, 
                                           Social_group = Social_group, 
                                           s = s,
                                           Info = Info)
          
          #Replace updated information (from Group) to original dataframe
          Agent_names <- Social_group$AgentNo
          for(x in Agent_names){
            
            #Check where the agent is in the original dataframe (AgentNo in Agents)
            if(Social_group$AgentNo[x] %in% Agents$AgentNo){
              #If true, replace old data new updated data
              Agents[Agents$AgentNo == x,] <- Social_group[Social_group$AgentNo == x,]
              
          }
        } # Done merging
      
      } # Reject integrate social
    } # Done integrating rejection social
  } # Done looping through social group
  } #Agents valence > Agents Share
  
  
      } # Social posting == TRUE 
    } # Done looping through agents
    #Visualization:
    if(t %% Plot_freq == 0 | t == 1){

    plot <- Agents %>%
    ggplot(aes(x, y, color = Group)) + geom_point(aes(size = Share)) + xlim(-1, 1) + ylim(-1, 1) + theme(legend.position = "None") + ggtitle(paste("TickNo", t)) +
      scale_size(trans = 'reverse')
    
    print(plot)
    }
  } # Done looping through time
  Agents$No_of_Shares <- Agents$No_of_Shares - 1 # Subtracting one so it's the correct No_of_Shares
  
  return(Agents)
}
300/7

Lat_reject <- 1.5
Lat_reject_social <- 1.5# Maybe it should be the double of lat of accept. Bigger latitude of acceptance means the agents will integrate more information they disagree with, and reinforce their current position. 
Agents20 <- GenPop(nPop = 300, nMemo = 20, Lat_accept = 0.3, Sharpness = 5, nGroups = 7)

#Where do the agents start the simulation (default (0,0))
Agents20[,2] <- runif(nrow(Agents20), -1, 1) #Random x coordinates
Agents20[,3] <- runif(nrow(Agents20), -1, 1) #Random y coordinates

Out <- ABM1(Agents = Agents20, Iterations = 100, Plot_freq = 10, Bias = "None", Social_posting = TRUE) # nmemo 20, ngroups 7, bias none, npop 100, lat reject 0.7, lat accept 0.3

Out1 <- ABM1(Agents = Agents20, Iterations = 100, Plot_freq = 10, Bias = "None", Social_posting = TRUE) # nmemo 20, ngroups 4, bias none, npop 100, lat reject 0.7, lat accept 0.3, iterations 100, sharpness 20

Out2 <- ABM1(Agents = Agents20, Iterations = 100, Plot_freq = 10, Bias = "Positive", Social_posting = TRUE) # nmemo 20, ngroups 4, bias Positive, npop 100, lat reject 0.7, lat accept 0.3, iterations 100, sharpness 20

Out3 <- ABM1(Agents = Agents20, Iterations = 100, Plot_freq = 10, Bias = "Positive", Social_posting = TRUE) # nmemo 20, ngroups 5, bias Positive, npop 100, lat reject 1, lat accept 0.3, iterations 100, sharpness 5

Out4 <- ABM1(Agents = Agents20, Iterations = 100, Plot_freq = 10, Bias = "Positive", Social_posting = TRUE) # nmemo 10, ngroups 10, bias Positive, npop 200, lat reject 1, lat accept 0.5, iterations 100

Out5 <- ABM1(Agents = Agents20, Iterations = 100, Plot_freq = 10, Bias = "Positive", Social_posting = TRUE) #nPop = 200, nMemo = 20, Lat_accept = 0.3, Sharpness = 5, nGroups = 7, lat reject 0.7

Out6 <- ABM1(Agents = Agents20, Iterations = 100, Plot_freq = 10, Bias = "Positive", Social_posting = TRUE) #nPop = 200, nMemo = 20, Lat_accept = 0.6, Sharpness = 5, nGroups = 7, lat reject 0.7

Out7 <- ABM1(Agents = Agents20, Iterations = 100, Plot_freq = 10, Bias = "Positive", Social_posting = TRUE) #nPop = 200, nMemo = 20, Lat_accept = 0.2, Sharpness = 5, nGroups = 7, lat reject 0.7

Out8 <- ABM1(Agents = Agents20, Iterations = 100, Plot_freq = 10, Bias = "Positive", Social_posting = FALSE) #nPop = 200, nMemo = 20, Lat_accept = 0.2, Sharpness = 5, nGroups = 7, lat reject 0.7

Out9 <- ABM1(Agents = Agents20, Iterations = 400, Plot_freq = 10, Bias = "Positive", Social_posting = TRUE) #nPop = 100, nMemo = 20, Lat_accept = 0.3, Sharpness = 5, nGroups = 5, bias positive

Out10 <- ABM1(Agents = Agents20, Iterations = 200, Plot_freq = 10, Bias = "None", Social_posting = TRUE) #nPop = 100, nMemo = 20, Lat_accept = 0.3, Sharpness = 5, nGroups = 5, bias None

Out11 <- ABM1(Agents = Agents20, Iterations = 300, Plot_freq = 10, Bias = "None", Social_posting = TRUE) #nPop = 100, nMemo = 15, Lat_accept = 0.3, Sharpness = 5, nGroups = 5, bias None

Out12 <- ABM1(Agents = Agents20, Iterations = 300, Plot_freq = 10, Bias = "None", Social_posting = TRUE) #nPop = 300, nMemo = 20, Lat_accept = 0.3, Sharpness = 5, nGroups = 7, bias None

Out13 <- ABM1(Agents = Agents20, Iterations = 300, Plot_freq = 10, Bias = "None", Social_posting = FALSE)


Out11 %>%
    ggplot(aes(x, y, color = Group)) + geom_point(aes(size = Share)) + xlim(-1, 1) + ylim(-1, 1) + theme(legend.position = "None") + ggtitle(paste("TickNo", 100)) +
      scale_size(trans = 'reverse')

Out3 %>% 
  ggplot(aes(No_of_Shares, color = Group)) + geom_density()


Out3 %>% 
  ggplot(aes(Share, color = Group)) + geom_density()

Out3 %>%
  count(Group, sort = T) #Group size might change how often a group shares.

Out12 %>% 
  group_by(Group) %>% 
  summarise(mean_no_share = mean(No_of_Shares),
            sd_no_share = sd(No_of_Shares),
            mean_valence = mean(Valence),
            sd_valence = sd(Valence))

mean(Out12$Valence)
mean(Out13$Valence)

Out12 %>% 
  group_by(Group) %>% 
  summarise(mean_x = mean(x)) %>% 
  ggplot(aes(x = Group, y = mean_x, fill = Group)) +
  geom_bar(stat = "identity") + 
    theme_classic() +
    labs(
        x = "Group",
        y = "Mean X",
        title = paste(
            "Mean X by group"
        )
    )

Out12 %>% 
  group_by(Group) %>% 
  summarise(mean_y = mean(y)) %>% 
  ggplot(aes(x = Group, y = mean_y, fill = Group)) +
  geom_bar(stat = "identity") + 
    theme_classic() +
    labs(
        x = "Group",
        y = "Mean Y",
        title = paste(
            "Mean Y by group"
        )
    )

Out13 %>% 
  group_by(Group) %>% 
  summarise(mean_valence = mean(Valence)) %>% 
  ggplot(aes(x = Group, y = mean_valence, fill = Group)) +
  geom_bar(stat = "identity") + 
    theme_classic() +
    labs(
        x = "Group",
        y = "Mean Valence",
        title = paste(
            "Mean valence by group"
        )
    )

Out3 %>% 
  ggplot(aes(Valence, color = Group)) + geom_boxplot()

Out9 %>% 
  ggplot(aes(x, color = Group)) + geom_boxplot()

Out13 %>% 
  ggplot(aes(x)) + geom_density() + ggtitle("Clustering of agents across X")

Out13 %>% 
  ggplot(aes(y)) + geom_density() + ggtitle("Clustering of agents across Y")
  

mean(Out1$x)
sd(Out1$x)
sd(Out1$y)


#For tomorrow: 
# 1. Find a way to model trustworthiness of information, 
# 2. figure out how Lorenz used information popularity, 
# 3. find a way to turn on social media, 
# 4. play with distributions of information valence (more likely positive/negative), 
# 5. see if I can visually see the confirmation bias that is modeled into the integration theory
# 6. Output??? Mean + sd of location over time?



```
 

  
 
```{r}

##Doing analysis on several datasets
Out$Social_posting <- 0
Out4$Social_posting <- 1

SP_contrast <- rbind(Out, Out4)
str(SP_contrast)
SP_contrast$Social_posting <- as.factor(SP_contrast$Social_posting)
SP_contrast$AgentNo <- as.factor(SP_contrast$AgentNo)

#Let's try to model valence as outcome
SP_contrast %>% 
  ggplot(aes(Valence)) + geom_density(aes(color = Social_posting)) #Semi normal distr.

SP_contrast %>% 
  group_by(Social_posting) %>% 
  summarise(mean_valence = mean(Valence),
            sd_valence = sd(Valence))

f1 <- bf(Valence ~ Social_posting + (1 | AgentNo))

get_prior(f1, data = SP_contrast, family = gaussian())

f1_prior <- c(
  prior(normal(0, 0.1), class = b), # No diff
  prior(student_t(3, 0, 2.5), class = sd), #Default
  prior(student_t(3, 0, 2.5), class = sigma) #Default
)

m1_prior <- brm(
  f1,
  data = SP_contrast,
  family = gaussian(),
  prior = f1_prior,
  sample_prior = "only",
  chains = 2,
  cores = 4,
  backend="cmdstanr",
  threads = threading(2),
  )

pp_check(m1_prior, nsamples = 50)


m1_post <- brm(
  f1,
  data = SP_contrast,
  family = gaussian(),
  prior = f1_prior,
  sample_prior = T,
  chains = 2,
  cores = 4,
  backend="cmdstanr",
  threads = threading(2),
  )

pp_check(m1_post, nsamples = 50)

conditional_effects(m1_post)

posterior_gauss <- posterior_samples(m1_post)
summary(m1_post)
colnames(posterior_gauss)

ggplot(posterior_gauss) + #Plotting the beta prior + posterior
  theme_classic() +
  geom_density(aes(prior_b), fill="red", alpha=0.3) +
  geom_density(aes(b_Social_posting1), fill="blue", alpha=0.5)

ggplot(posterior_gauss) + #Plotting the sigma prior + posterior
  theme_classic() +
  geom_density(aes(prior_sigma), fill="red", alpha=0.3) +
  geom_density(aes(sigma), fill="blue", alpha=0.5)

##SAMPLING QUALITY
mcmc_trace(m1_post, 
           pars = c("b_Social_posting1", 
                    "sd_AgentNo__Intercept",
                    'sigma')) +
  theme_classic() 

mcmc_rank_overlay(m1_post,
                  pars = c("b_Social_posting1", 
                    "sd_AgentNo__Intercept",
                    'sigma')) + 
  theme_classic()


```

```{r}
##Frequentist stats

model <- lmerTest::lmer(Valence ~ Social_posting + (1|AgentNo), data = SP_contrast)

model1 <- lmerTest::lmer(Valence ~ Social_posting + Share + (1|AgentNo), data = SP_contrast)
summary(model); summary(model1)
?isSingular
?t.test
t.test(Valence, )

df <- (matrix(0, nrow = 200, ncol = 0))

df$Social <- SP_contrast %>%
  filter(Social_posting == 1) %>% 
  select(Valence)

df$nonSocial <- SP_contrast %>%
  filter(Social_posting == 0) %>% 
  select(Valence)

df$ID <- 1:200
df <- as.data.frame(df)
df <- rename(df, Social = Valence)
df <- rename(df, nonSocial = Valence.1)
#Now it's in the format to do a simple t.test
t.test(Valence ~ Social_posting, paired = T, data = SP_contrast, var.equal = T)


```

